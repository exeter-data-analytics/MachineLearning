[
["index.html", "Introduction to Machine Learning Preface Prerequisites Learning outcomes Recommended reading Software packages Data files", " Introduction to Machine Learning Chris Yeomans and Jiangjiao Xu 05 May 2020 Preface This course is based on original material by JJ Valetta from Easter 2019 An introductory workshop on the field of machine learning. The focus will be on how to use these methods in practice using R and Python, rather than on the rigorous underlying mathematics. The target audience is anyone who wants to know what machine learning is, what problems it can solve and how we can solve them in practice using R or Python. Prerequisites Programming basics in either R or Python Learning outcomes Understand the key concepts and terminology used in the field of machine learning Build predictive models for clustering, regression and classification problems Apply machine learning algorithms in R/Python to a variety of real-world datasets Recognise practical issues in data-driven modelling Recommended reading I highly recommend the following books: An Introduction to Statistical Learning The Elements of Statistical Learning Pattern Recognition and Machine Learning Machine Learning: A Probabilistic Perspective Software packages R: caret Python: scikit-learn Please make sure to read the documentation of any machine learning algorithm before using it! Data files All data files can be downloaded as a ZIP file from here. "],
["introduction.html", "1 Introduction 1.1 Motivation 1.2 What is machine learning? 1.3 What problems can machine learning solve? 1.4 Types of machine learning methods 1.5 Statistics and Machine Learning 1.6 Terminology 1.7 A bird’s-eye view of building machine learning systems", " 1 Introduction 1.1 Motivation Scientists are nowadays faced with an unprecedented amount of complex and big data sets (e.g. high-throughput sequencing, GIS (Geographical Information System) data, biomedical imaging and social media data). These data sets are very challenging to analyse due to nonlinear dependencies, mixed data sources and high-dimensionality. They often fail to conform to the assumptions of classical statistical methods. Hand-in-hand with the rise of computational power, machine learning (ML), has matured into a field of its own, to specificially extract knowledge from these challenging datasets. 1.2 What is machine learning? A machine (an algorithm/model) improves its performance (predictive accuracy) in achieving a task (e.g classifying the content of an image) from experience (data). The automatic discovery of patterns and regularities in data. 1.3 What problems can machine learning solve? Object recognition Biomarker discovery in genomics Navigation of autonomous vehicles Fraud detection … and much more! 1.4 Types of machine learning methods Unsupervised learning Unsupervised learning methods uncover structure in unlabelled data. Structure means patterns in the data that are sufficiently different from pure unstructured noise. Structure can be discovered by: Determining the distribution of the data using density estimation techniques Visualising the data after dimensionality reduction (e.g Principal Component Analysis (PCA)) Identifying groups of observations sharing similar attributes using clustering methods Supervised learning Akin to traditional statistical models (e.g generalised linear models) supervised learning methods discover the relationship between an outcome and a set of explanatory variables. Using training data, the model learns the mapping (predictive model) between a set of features and a: Continuous outcome - regression Categorical variable - classification Semi-supervised learning Similar to supervised learning, however these methods also make use of unlabelled data to improve the model’s predictive performance. Reinforcement learning These methods mimic the way humans learn new games or skills (e.g riding a unicycle). The machine/model explores different actions that maximise a reward (e.g score achieved in a game or time spent upright on a unicyle) by a process of trial and error. There is an inherent tradeoff between exploration (trying out new actions) and exploitation (use actions that already give reasonable results). Here’s a nice example. In this introductory workshop we will only focus on unsupervised and supervised learning methods. 1.5 Statistics and Machine Learning There is substantial overlap between the fields of statistics and machine learning. Some high-profile academics, such as Robert Tibshirani, even argue that ML is merely “glorified statistics”. He even provides a handy glossary. We will not engage in a philosophical debate here, rather we focus on a pragmatic comparison between these two schools of thought, which evolved from different research areas and tackled different problems. Statistics Machine learning Philosophy provide humans with a set of data analysis tools replace humans in the processing of data Focus what is the relationship between the data and the outcome? how can we predict the outcome using the data? Inference how was the observed data generated? what do the model parameters mean in practice? typically only care about predictions and not what the model parameters mean Learning use all of the observed data to perform inference at the population-level use training data, then use testing data to perfom individual-level predictions Validation measures of fit (\\(R^2\\), chi-square test, etc.) and suitability of inferred parameters predictive performance measures (root mean squared error (RMSE), area under the ROC cuve (AUC), etc.) computed on “unseen” data (generalisation) Model selection adjusted measures of fit (adjusted \\(R^2\\), \\(C_p\\) statistic, Aikake information criterion, etc.) Cross-validation and out-of-bag errors The line between ML and statistics is blurry at best. Personally, I do not find engaging in heated debates between the two fields to be healthy. Both fields complement each other and as the late Leo Breiman puts it: The best solution could be an algorithmic model (machine learning), or maybe a data model, or maybe a combination. But the trick to being a scientist is to be open to using a wide variety of tools. — Leo Breiman 1.6 Terminology The jargon used in ML can be daunting at first. The table below summarises the most commonly encountered terms and their synonyms: Training dataset data used to train a set of machine learning models Validation dataset data used for model selection and validation i.e to choose a model which is complex enough to describe the data “well” but not more complex Testing dataset data not used when building the machine learning model, but used to evaluate the model’s performance on previously unseen data (generalisation error) Features the covariates/predictors/inputs/attributes used to train the model Training error the model’s performance evaluated on the training data (also known as in-sample or resubstitution error) Testing error the model’s performance evaluated on the testing data (also known as out-of-sample or generalisation error) 1.7 A bird’s-eye view of building machine learning systems Health warning: Real-life data is very messy. You will end up spending most of your time preparing your data into a format amenable for exploration/modelling. Do not despair, in this workshop you will be provided with clean data sets that can be used straightaway. Nevertheless, if you have not attended a course on data wrangling and visualisation yet, I would strongly recommend doing TJ McKinley’s course. "],
["clustering.html", "2 Clustering 2.1 Motivation 2.2 What is clustering? 2.3 What problems can clustering solve? 2.4 Types of clustering methods 2.5 Similarity measures 2.6 The Iris dataset 2.7 \\(k\\)-means 2.8 Agglomerative hiearchical clustering 2.9 Gaussian mixture model (GMM) 2.10 Determining the “correct” number of clusters 2.11 Tasks", " 2 Clustering 2.1 Motivation The image above is from a microarray experiment 1. The intensity of each dot represents gene expression of a single gene (how “active” the gene is) for a particular individual/sample. The resultant data is therefore a big matrix of numbers, where each column represents a gene and each row an individual/sample. There are two questions of interest: Which genes are co-regulated, that is, behave in the same way? Which individuals are similar to each other, that is, have a similar gene expression profile? In both cases we want to discover some underlying structure in unlabelled data. Structure means patterns in the data that are sufficiently different from pure unstructured noise. Here we introduce clustering, a class of unsupervised learning methods that try to answer these questions. 2.2 What is clustering? The goal of clustering is to find groups that share similar properties. The data in each group should be similar (minimise intracluster distance), but each cluster should be sufficiently different (maximise intercluster similarity). 2.3 What problems can clustering solve? Clustering is particularly useful in applications where labelling the data is very time consuming/expensive. Gene expression: discovering co-regulated genes. Biological systematics: finding organisms sharing similar attributes. Computer vision: segmenting a digital image for object recognition. Epidemiology: identifying geographical clusters of diseases. Medical imaging: differentiating between tissues. Mathematical chemistry: grouping compounds by topological indices. Market basket analysis: determining which group of items tend to be bought together. Cybersecurity: detecting fraudulent activity. … and much more! 2.4 Types of clustering methods Partitional: the feature space is partitioned into \\(k\\) regions e.g \\(k\\)-means. Hierarchical: iteratively merging small clusters into larger ones (agglomerative) or breaking large clusters into smaller ones (divisive). Distribution-based: fit \\(k\\) multivariate statistical distributions e.g Gaussian mixture model (GMM). 2.5 Similarity measures Most clustering methods rely on distance metrics that quantify how close two observations are. There are several ways to define this distance, which has a direct effect on the clustering result. The Euclidean distance (think Pythagoras theorem) is depicted below, together with the Manhatttan distance (named after the journey a taxi has to follow in grid-like streets of cities like Manhattan). The correlation coefficient is also another popular way to measure similarity. There are various other distance metrics, please see dist in R or pdist in Python. In this introductory workshop we will focus on continuous features, but be aware that distance measures for categorical variables exists, such as, the Jaccard index, Gower distance and polychoric correlation. 2.6 The Iris dataset To showcase some of the clustering methods, we will use the popular Iris dataset. The data set consists of 50 samples from three species of Iris flower (I. setosa, I. virginica and I. versicolor). Each flower is quantified by four measurements, length and width of sepal and petal. Let us load this dataset: R Python # The iris dataset is preloaded in R head(iris) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa # The iris dataset is available from the sci-kit learn package from sklearn import datasets iris = datasets.load_iris() # Print the first 6 rows # Sepal Length, Sepal Width, Petal Length, Petal Width iris.data[:6, ] array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2], [5.4, 3.9, 1.7, 0.4]]) 2.7 \\(k\\)-means Arguably the most widely used partitioning clustering method. The feature space is divided into \\(k\\) regions as follows: Select \\(k\\) centroids at random. Compute the Euclidean distance between centroids and each data point. Assign each data point to the closest centroid. Compute new centroids; the average of all data points in that cluster. Repeat steps 2 to 4 until data points remain in the same cluster or some maximum number of iterations reached. Note: \\(k\\)-means clustering should only be used with continuous data! For visualisation purposes let’s just use two features of the Iris dataset; sepal length and petal width. R Python # Fit k-means model k &lt;- 3 mdl &lt;- kmeans(x=iris[, c(1, 4)], centers=k) # Associate a colour with each cluster library(RColorBrewer) COL &lt;- seq(k) names(COL) &lt;- brewer.pal(n=k, &#39;Set1&#39;) # Plot results plot(iris[, 4], iris[, 1], pch=19, col=names(COL[mdl$cluster]), xlab=&#39;Petal width (cm)&#39;, ylab=&#39;Sepal length (cm)&#39;) # Fit k-means model from sklearn.cluster import KMeans k = 3 mdl = KMeans(n_clusters=k, n_jobs=-1) # -1 uses all cores mdl.fit(X=iris.data[:, [0, 3]]) # Associate a colour with each cluster from palettable.colorbrewer.qualitative import Set1_3 colDict = {0: Set1_3.hex_colors[0], 1: Set1_3.hex_colors[1], 2: Set1_3.hex_colors[2]} myCol = [colDict[i] for i in mdl.labels_] # Plot results import matplotlib.pyplot as plt plt.scatter(iris.data[:, 3], iris.data[:, 0], c=myCol) plt.xlabel(&#39;Petal width (cm)&#39;) plt.ylabel(&#39;Sepal length (cm)&#39;) Pros Cons Simple and intuitive \\(k\\) needs to be specified a priori Computationally inexpensive/fast Only applicable for continuous data where a mean is defined No guarantee of a global optimum solution 2.8 Agglomerative hiearchical clustering In agglomerative hierarchical clustering small clusters are iteratively merged into larger ones. The clustering strategy is as follows: Assign each datum as its own cluster. Compute the distance between each cluster. Merge the closest pair into a single cluster. Repeat steps 2 to 3 until all clusters are merged together. Step 3 is key, the distance metric and linkage function dictate the final result. The linkage function specifies how the inter-cluster distance is computed. There are various options: Centroid: mean of data points (same as in \\(k\\)-means). Single: distance between closest pair of points. Complete: distance between furthest pair of points. Average: mean pairwise distance between all points. The distance can be computed using any similarity measure introduced previously. R Python # Compute distance matrix d &lt;- dist(x=iris[, c(1, 4)], method=&#39;euclidean&#39;) # Perform agglomerative hierarchical clustering # Use &#39;average&#39; link function mdl &lt;- hclust(d=d, method=&#39;average&#39;) # Plot resultant dendrogram plot(mdl, cex=0.6) from scipy.spatial.distance import pdist from scipy.cluster.hierarchy import linkage, dendrogram # Compute distance matrix d = pdist(X=iris.data[:, [0, 3]], metric=&quot;euclidean&quot;) # Perform agglomerative hierarchical clustering # Use &#39;average&#39; link function mdl = linkage(d, method=&#39;average&#39;) # Plot resultant dendrogram plt.figure(figsize=(9,6)) dendrogram(mdl) plt.show() The number at the end of each branch corresponds to the observation row number. Pros Cons No need to specify \\(k\\) Can be computationally expensive Sub-groups within larger clusters can be easily identified Interpretation is subjective. Where should we draw the line? Dendrograms let us visualise results irrespective of number of features Choice of distance method and linkage function can significantly change the result 2.9 Gaussian mixture model (GMM) The GMM is a simple but powerful model that performs clustering via density estimation. The features’ histogram is modelled as the sum of multiple multivariate Gaussian distributions. Suppose we only had access to one feature, a GMM with \\(k=2\\) would look something like this: The blue dashed lines represent the two individual univariate Gaussians, whilst the black line depicts the combined model. We can extend this to more features by using multivariate Gaussians. Mathematically this can be expressed as follows: \\[ p(x) = \\sum_{i=1}^k \\pi_i \\mathcal{N}(x|\\mu_i, \\Sigma_i)\\\\ \\sum_{i=1}^k \\pi_i = 1 \\] The Expectation-Maximisation (EM) algorithm is used to estimate the parameters \\(\\pi_i\\) (known as mixing coefficients), \\(\\mu_i\\) and \\(\\Sigma_i\\). R Python library(mclust) Package &#39;mclust&#39; version 5.4.5 Type &#39;citation(&quot;mclust&quot;)&#39; for citing this R package in publications. # Fit Gaussian Mixture Model k &lt;- 3 # no. of clusters mdl &lt;- Mclust(data=iris[, c(4, 1)], G=3) # Plot results plot(mdl, what=&#39;classification&#39;, xlab=&#39;Petal width (cm)&#39;, ylab=&#39;Sepal length (cm)&#39;) import numpy as np from matplotlib.colors import LogNorm from sklearn.mixture import GaussianMixture as GMM # Fit Gaussian Mixture Model k = 3 # no. of clusters mdl = GMM(n_components=3) mdl.fit(X=iris.data[:, [3, 0]]) # Compute probability distribution function at each point on a gird x = np.linspace(np.min(iris.data[:, 3]), np.max(iris.data[:, 3]), 100) y = np.linspace(np.min(iris.data[:, 0]), np.max(iris.data[:, 0]), 100) X, Y = np.meshgrid(x, y) XX = np.array([X.ravel(), Y.ravel()]).T Z = -mdl.score_samples(XX) Z = Z.reshape(X.shape) # Plot results hPlot = plt.contour(X, Y, Z, norm=LogNorm(), levels=np.logspace(0, 3, 10)) plt.colorbar(hPlot, shrink=0.8, extend=&#39;both&#39;) plt.scatter(iris.data[:, 3], iris.data[:, 0]) plt.xlabel(&#39;Petal width (cm)&#39;) plt.ylabel(&#39;Sepal length (cm)&#39;) Pros Cons Intuitive interpretation \\(k\\) needs to be specified a priori Computationally inexpensive Strong assumption on the distribution of the feature space (multivariate Gaussian) No guarantee of a global optimum solution Fails when number of features is much greater than observations GMMs offer a “soft” clustering approach, where every observation is part of every cluster but with varying levels of membership. 2.10 Determining the “correct” number of clusters One of the biggest questions when it comes to clustering is “How many clusters do I have?”. The number of clusters \\(k\\) cannot be determined exactly, because the observations are unlabelled, so \\(k\\) is inherently ambiguous. Moreover, similarity is quite subjective and often we cannot define a clear cut-off. For example, suppose that as part of a public health exercise we want to cluster a large group of individuals based on their health. Health is a multifaceted concept and cannot be observed directly; instead we measure various biomarkers, like body mass index (BMI), cholesterol levels, body composition, resting metabolic rate, etc. Although we would be able to differentiate between individuals at the two extremes (i.e athelete vs couch potato), most people will sit somewhere on a continuum. There isn’t a clear “line”, that once crossed an individual goes from being healthy to a bit unhealthy or moderately unhealthy etc. The number of clusters is therefore somewhat dictated by the problem at hand and the type of questions we’re trying to answer. Nevertheless, there are various metrics that one can use to estimate the underlying number of clusters: Recall that the objective of clustering is to minimise the intracluster distance and maximise the intercluster similarity. Thus, we can plot the within and between clusters sum-of-squares distances as a function of \\(k\\). As we increase the number of clusters, there will be a point where the sum-of-squares distances will only change marginally, that is, adding more clusters does not improve these metrics significantly. The number of clusters is chosen to be the point at which the curve “plateaus” (5 in the synthetic example below). This is known as the “elbow criterion”. Please refer to the R or Python documentation on how to access these metrics. The silhouette width quantifies how similar an observation is to its own cluster compared to other clusters. This measure ranges from -1 (not compatible with that cluster) to 1 (extremely likely to be part of that cluster). The suggested configuration is the one that maximises the average silhouette width (3 in the synthetic example below). Please refer to the R or Python documentation on how to compute and plot these metrics. For distribution-based methods, choosing \\(k\\) can be framed as a model selection problem. We can plot the Akaike Information Criterion (AIC), Bayesian Information Criterior (BIC) or other information criterion measures. As we increase the number of clusters, there will be a point where the model fit will only improve marginally or start to decrease. The number of clusters is chosen to be the point at which the curve “plateaus” (the “elbow criterion”; 5 in the synthetic example below). Please refer to the R or Python documentation on how to compute these metrics. There are myriad other metrics available in the literature, some related to specific clustering algorithms. You will also encounter methods that claim to automatically discover the optimal number of clusters for you. Although, this can be true in a mathematical sense, this estimate will still be based on various underlying assumptions and hyperparameters. These cluster validity measures only give us a ballpark range for the “correct” number of clusters. Ultimately one needs to make use of prior knowledge to determine whether the number of clusters are practically relevant and if they make sense. For example, how many different phenotypes are you expecting in your population? 2.11 Tasks 2.11.1 Simulated data Let’s start to get a feel for these clustering algorithms by simulating some data: R Python library(MASS) # mvrnorm (multivariate normal) # Set simulation parameters N &lt;- 50 # no. of data points in each cluster covMatrix &lt;- matrix(data=c(1, 0, 0, 2), nrow=2) # Simulate clusters (assume same covariances for now) set.seed(1034) # to reproduce results clustA &lt;- mvrnorm(n=N, mu=c(6, 4), Sigma=covMatrix) clustB &lt;- mvrnorm(n=N, mu=c(3, 9), Sigma=covMatrix) clustC &lt;- mvrnorm(n=N, mu=c(9, 9), Sigma=covMatrix) # Join all the data together and plot xTrain &lt;- rbind(clustA, clustB, clustC) plot(xTrain[, 1], xTrain[, 2], pch=19, xlab=&#39;Feature 1&#39;, ylab=&#39;Feature 2&#39;) from numpy.random import multivariate_normal, seed # Set simulation parameters N = 50 # no. of data points in each cluster covMatrix = np.array([[1, 0], [0, 2]], dtype=&#39;float&#39;) # Simulate clusters (assume same covariances for now) seed(1034) # to reproduce results clustA = multivariate_normal(mean=np.array([6, 4]), cov=covMatrix, size=N) clustB = multivariate_normal(mean=np.array([3, 9]), cov=covMatrix, size=N) clustC = multivariate_normal(mean=np.array([9, 9]), cov=covMatrix, size=N) # Join all the data together and plot xTrain = np.vstack((clustA, clustB, clustC)) plt.figure(figsize=(6,6)) plt.scatter(xTrain[:, 0], xTrain[:, 1]) plt.xlabel(&#39;Feature 1&#39;) plt.ylabel(&#39;Feature 2&#39;) Task Perform \\(k\\)-means clustering by allowing \\(k\\) to vary from 2 to 6 . Plot the intra and intercluster sum-of-squares as a function of \\(k\\) and deduce the “true” number of underlying clusters. Show Solution Solution R Python # Initialise some variables kRange &lt;- seq(from=2, to=6, by=1) intra &lt;- rep(NA, length(kRange)) # intracluster sum-of-squares inter &lt;- rep(NA, length(kRange)) # intercluster sum-of-squares # Loop across desired range of ks for (k in kRange) { mdl &lt;- kmeans(x=xTrain , centers=k) intra[k-1] &lt;- mdl$tot.withinss # it’s (k-1) because k starts from 2 inter[k-1] &lt;- mdl$betweenss } # Plot inter/intercluster sum-of-squares as a function of $k$ yMin &lt;- min(c(intra , inter)) yMax &lt;- max(c(intra , inter)) plot(kRange, inter, type=&quot;o&quot;, pch=1, col=&quot;blue&quot;, lwd=3, lty=1, ylim=c(yMin, yMax), xlab=&quot;k&quot;, ylab=&#39;Sum-of-squares&#39;) points(kRange, intra, type=&quot;o&quot;, pch=1, col=&quot;red&quot;, lwd=3) legend(&quot;right&quot;, c(&quot;inter-cluster&quot;, &quot;intra-cluster&quot;), bty=&quot;n&quot;, col=c(&quot;blue&quot;, &quot;red&quot;), pch=1, lwd=3, lty=1) The “correct” number of clusters is approximately 3. This matches our expectation. from sklearn.preprocessing import scale # Initialise some variables kRange = range(2, 7) intra = np.empty(len(kRange)) # intracluster sum-of-squares inter = np.empty(len(kRange)) # intercluster sum-of-squares # Loop across desired range of ks for k in kRange: mdl = KMeans(n_clusters=k) mdl.fit(X=xTrain) intra[k-2] = mdl.inertia_ # it’s (k-2) because k starts from 2 inter[k-2] = np.sum(scale(xTrain, with_std=False)**2) - mdl.inertia_ # Plot inter/intercluster sum-of-squares as a function of $k$ plt.figure(figsize=(5,5)) plt.plot(kRange, inter, &#39;bo-&#39;, label=&#39;inter-cluster&#39;) plt.plot(kRange, intra, &#39;ro-&#39;, label=&#39;intra-cluster&#39;) plt.legend(loc=&#39;center right&#39;) plt.xlabel(&#39;k&#39;) plt.ylabel(&#39;Sum-of-squares&#39;) The “correct” number of clusters is approximately 3. This matches our expectation. Task Fit a Gaussian Mixture Model (GMM) by allowing \\(k\\) to vary from 2 to 6 . Plot the AIC and BIC as a function of \\(k\\) and deduce the “true” number of underlying clusters. Show Solution Solution R Python # Initialise some variables kRange &lt;- seq(from=2, to=6, by=1) AIC &lt;- rep(NA, length(kRange)) # Akaike information criterion BIC &lt;- rep(NA, length(kRange)) # Bayesian information criterion # Loop across desired range of ks for (k in kRange) { mdl &lt;- Mclust(data=xTrain, G=k) AIC[k-1] &lt;- 2*mdl$loglik - 2*mdl$df BIC[k-1] &lt;- mdl$bic } # Plot inter/intercluster sum-of-squares as a function of $k$ yMin &lt;- min(c(AIC , BIC)) yMax &lt;- max(c(AIC , BIC)) plot(kRange, AIC, type=&quot;o&quot;, pch=1, col=&quot;blue&quot;, lwd=3, lty=1, ylim=c(yMin, yMax), xlab=&quot;k&quot;, ylab=&#39;Information criterion&#39;) points(kRange, BIC, type=&quot;o&quot;, pch=1, col=&quot;red&quot;, lwd=3) legend(&quot;right&quot;, c(&quot;AIC&quot;, &quot;BIC&quot;), bty=&quot;n&quot;, col=c(&quot;blue&quot;, &quot;red&quot;), pch=1, lwd=3, lty=1) The “correct” number of clusters is approximately 3. This matches our expectation. # Initialise some variables kRange = range(2, 7) AIC = np.empty(len(kRange)) # Akaike information criterion BIC = np.empty(len(kRange)) # Bayesian information criterion # Loop across desired range of ks for k in kRange: mdl = GMM(n_components=k) mdl.fit(X=xTrain) AIC[k-2] = -mdl.aic(xTrain) BIC[k-2] = -mdl.bic(xTrain) # Plot inter/intercluster sum-of-squares as a function of $k$ plt.figure(figsize=(6,6)) plt.plot(kRange, AIC, &#39;bo-&#39;, label=&#39;AIC&#39;) plt.plot(kRange, BIC, &#39;ro-&#39;, label=&#39;BIC&#39;) plt.legend(loc=&#39;center right&#39;) plt.xlabel(&#39;k&#39;) plt.ylabel(&#39;Information criterion&#39;) The “correct” number of clusters is approximately 3. This matches our expectation. Task Perform the above analysis on several sets of simulated data by changing the mean and covariance matrix of each simulated cluster. Try bringing the clusters closer together and then push them further apart. What happens to intra/intercluster distance and AIC/BIC plots? Show Solution Solution As clusters move closer together, the “elbow” becomes less pronounced due to the large overlap between clusters. The “correct” number of clusters will inevitably become more ambiguous. As clusters move further apart, the “elbow” becomes more pronounced, making it easier to determine the “correct” number of clusters. Note, however, that we could still have sub-groups within a particular cluster; it’s just that the within-cluster dissimilarity would be subtle compared to the across-clusters dissimilarity. For example, if one were to cluster a large set of images containing dogs, cats and horses, you would expect to find three distinct clusters. However, we know that within the dogs cluster you’d also have sub-clusters related to different species of dogs. 2.11.2 Gene expression The file gene_expression.csv (all workshop datasets are available here), contains the acute lymphoblastic leukaemia (ALL) dataset which was published in the following study (see here and here). The dataset contains normalised gene expression values (measured using microarray) for 128 patients and 12,625 genes. The patients were diagnosed with either a B- or T-cell acute lymphocytic leukaemia. Do not worry too much about the details (i.e what the genes are etc.), treat this dataset as a \\(G \\times N\\) matrix where \\(G\\) is the total number of genes and \\(N\\) is the number of patients. We have access to the labels, type and stage of the disease (e.g B2). Thus, we can easily assess how well the clustering algorithm is doing, as we expect the B’s and T’s to cluster together. R Python xTrain &lt;- read.csv(&#39;_data/gene_expression.csv&#39;, row.names=1) print(dim(xTrain)) [1] 12625 128 import pandas as pd xTrain = pd.read_csv(&#39;_data/gene_expression.csv&#39;, header=0, index_col=0) print(xTrain.shape) (12625, 128) Task Perform agglomerative hierarchical clustering on the 128 patients. Keep the distance method as euclidean, but change the linkage method (e.g single, average) and observe how the dendrogram changes. Show Solution Solution R Python linkMethods &lt;- c(&#39;single&#39;, &#39;complete&#39;, &#39;average&#39;) distMethod &lt;- &#39;euclidean&#39; distance &lt;- dist(as.matrix(t(xTrain), method=distMethod)) for (linkMethod in linkMethods) { mdl &lt;- hclust(distance , method=linkMethod) plot(mdl, cex=0.5, xlab=&#39;distance&#39;, main=paste0(distMethod, &#39; distance and &#39;, linkMethod, &#39; link function&#39;)) } linkMethods = [&#39;single&#39;, &#39;complete&#39;, &#39;average&#39;] distMethod = &#39;euclidean&#39; distance = pdist(X=xTrain.T, metric=distMethod ) for linkMethod in linkMethods: mdl = linkage(distance, method=linkMethod) plt.figure(num=linkMethod, figsize=(9,6)) dendrogram(mdl) plt.title(&#39;{0} distance and {1} link function&#39;.format(distMethod, linkMethod)) plt.show() Task Same as before, but now keep the linkage method as average and change the distance method (e.g euclidean, manhattan) and observe how the dendrogram changes. Show Solution Solution R Python linkMethod &lt;- &#39;average&#39; distMethods &lt;- c(&#39;euclidean&#39;, &#39;manhattan&#39;, &#39;canberra&#39;) for (distMethod in distMethods) { distance &lt;- dist(as.matrix(t(xTrain), method=distMethod)) mdl &lt;- hclust(distance , method=linkMethod) plot(mdl, cex=0.5, xlab=&#39;distance&#39;, main=paste0(distMethod, &#39; distance and &#39;, linkMethod, &#39; link function&#39;)) } linkMethod = &#39;average&#39; distMethods = [&#39;euclidean&#39;, &#39;cityblock&#39;, &#39;canberra&#39;] for distMethod in distMethods: distance = pdist(X=xTrain.T, metric=distMethod ) mdl = linkage(distance, method=linkMethod) plt.figure(num=distMethod, figsize=(9,6)) dendrogram(mdl) plt.title(&#39;{0} distance and {1} link function&#39;.format(distMethod, linkMethod)) plt.show() 2.11.3 Wine The file wine.csv contains chemical analysis data of wines grown in the same region in Italy but from three different cultivars (see here for details). R Python xTrain &lt;- read.csv(&#39;_data/wine.csv&#39;) print(head(xTrain)) WineType Alcohol MalicAcid Ash AlcalinityAsh Magnesium TotalPhenols 1 A 14.23 1.71 2.43 15.6 127 2.80 2 A 13.20 1.78 2.14 11.2 100 2.65 3 A 13.16 2.36 2.67 18.6 101 2.80 4 A 14.37 1.95 2.50 16.8 113 3.85 5 A 13.24 2.59 2.87 21.0 118 2.80 6 A 14.20 1.76 2.45 15.2 112 3.27 Flavanoids NonflavanoidPhenols Proanthocyanins ColorIntensity Hue 1 3.06 0.28 2.29 5.64 1.04 2 2.76 0.26 1.28 4.38 1.05 3 3.24 0.30 2.81 5.68 1.03 4 3.49 0.24 2.18 7.80 0.86 5 2.69 0.39 1.82 4.32 1.04 6 3.39 0.34 1.97 6.75 1.05 OD280_OD315 Proline 1 3.92 1065 2 3.40 1050 3 3.17 1185 4 3.45 1480 5 2.93 735 6 2.85 1450 xTrain = pd.read_csv(&#39;_data/wine.csv&#39;, header=0) print(xTrain.head()) WineType Alcohol MalicAcid ... Hue OD280_OD315 Proline 0 A 14.23 1.71 ... 1.04 3.92 1065 1 A 13.20 1.78 ... 1.05 3.40 1050 2 A 13.16 2.36 ... 1.03 3.17 1185 3 A 14.37 1.95 ... 0.86 3.45 1480 4 A 13.24 2.59 ... 1.04 2.93 735 [5 rows x 14 columns] There are thirteen variables (Alcohol, MalicAcid, etc.), together with WineType, which specifies the type of wine. Here, we are going to pretend we do not know that there are three types of wine, instead we’ll use clustering methods to uncover this information. One thing to notice with this data, is that the units vary greatly across variables. For example, Proline ranges from 278 to 1680, whilst MalicAcid ranges from 0.74 to 5.8. So first we need to normalise the data, so that they’re on a common scale. R Python wineType &lt;- xTrain$WineType # save for comparison xTrain &lt;- scale(xTrain[-1], center=TRUE, scale=TRUE) print(head(xTrain)) Alcohol MalicAcid Ash AlcalinityAsh Magnesium TotalPhenols [1,] 1.5143408 -0.56066822 0.2313998 -1.1663032 1.90852151 0.8067217 [2,] 0.2455968 -0.49800856 -0.8256672 -2.4838405 0.01809398 0.5670481 [3,] 0.1963252 0.02117152 1.1062139 -0.2679823 0.08810981 0.8067217 [4,] 1.6867914 -0.34583508 0.4865539 -0.8069748 0.92829983 2.4844372 [5,] 0.2948684 0.22705328 1.8352256 0.4506745 1.27837900 0.8067217 [6,] 1.4773871 -0.51591132 0.3043010 -1.2860793 0.85828399 1.5576991 Flavanoids NonflavanoidPhenols Proanthocyanins ColorIntensity Hue [1,] 1.0319081 -0.6577078 1.2214385 0.2510088 0.3611585 [2,] 0.7315653 -0.8184106 -0.5431887 -0.2924962 0.4049085 [3,] 1.2121137 -0.4970050 2.1299594 0.2682629 0.3174085 [4,] 1.4623994 -0.9791134 1.0292513 1.1827317 -0.4263410 [5,] 0.6614853 0.2261576 0.4002753 -0.3183774 0.3611585 [6,] 1.3622851 -0.1755994 0.6623487 0.7298108 0.4049085 OD280_OD315 Proline [1,] 1.8427215 1.01015939 [2,] 1.1103172 0.96252635 [3,] 0.7863692 1.39122370 [4,] 1.1807407 2.32800680 [5,] 0.4483365 -0.03776747 [6,] 0.3356589 2.23274072 from sklearn.preprocessing import scale wineType = xTrain[&#39;WineType&#39;] # save for comparison xTrain = scale(xTrain.drop(labels=&#39;WineType&#39;, axis=1), with_mean=True, with_std=True) print(xTrain) [[ 1.51861254 -0.5622498 0.23205254 ... 0.36217728 1.84791957 1.01300893] [ 0.24628963 -0.49941338 -0.82799632 ... 0.40605066 1.1134493 0.96524152] [ 0.19687903 0.02123125 1.10933436 ... 0.31830389 0.78858745 1.39514818] ... [ 0.33275817 1.74474449 -0.38935541 ... -1.61212515 -1.48544548 0.28057537] [ 0.20923168 0.22769377 0.01273209 ... -1.56825176 -1.40069891 0.29649784] [ 1.39508604 1.58316512 1.36520822 ... -1.52437837 -1.42894777 -0.59516041]] Task Perform \\(k\\)-means clustering by allowing \\(k\\) to vary from 1 to 10 . Plot the intra and intercluster sum-of-squares as a function of \\(k\\) and deduce the “true” number of underlying clusters. Show Solution Solution R Python # Initialise some variables kRange &lt;- seq(from=1, to=10, by=1) intra &lt;- rep(NA, length(kRange)) # intracluster sum-of-squares inter &lt;- rep(NA, length(kRange)) # intercluster sum-of-squares # Loop across desired range of ks for (k in kRange) { mdl &lt;- kmeans(x=xTrain , centers=k) intra[k] &lt;- mdl$tot.withinss inter[k] &lt;- mdl$betweenss } # Plot inter/intercluster sum-of-squares as a function of $k$ yMin &lt;- min(c(intra , inter)) yMax &lt;- max(c(intra , inter)) plot(kRange, inter, type=&quot;o&quot;, pch=1, col=&quot;blue&quot;, lwd=3, lty=1, ylim=c(yMin, yMax), xlab=&quot;k&quot;, ylab=&#39;Sum-of-squares&#39;) points(kRange, intra, type=&quot;o&quot;, pch=1, col=&quot;red&quot;, lwd=3) legend(&quot;topright&quot;, c(&quot;inter-cluster&quot;, &quot;intra-cluster&quot;), bty=&quot;n&quot;, col=c(&quot;blue&quot;, &quot;red&quot;), pch=1, lwd=3, lty=1) The “correct” number of clusters is approximately 3. This matches our expectation. from sklearn.preprocessing import scale # Initialise some variables kRange = range(1, 11) intra = np.empty(len(kRange)) # intracluster sum-of-squares inter = np.empty(len(kRange)) # intercluster sum-of-squares # Loop across desired range of ks for k in kRange: mdl = KMeans(n_clusters=k) mdl.fit(X=xTrain) intra[k-1] = mdl.inertia_ inter[k-1] = np.sum(scale(xTrain, with_std=False)**2) - mdl.inertia_ # Plot inter/intercluster sum-of-squares as a function of $k$ plt.figure(figsize=(5,5)) plt.plot(kRange, inter, &#39;bo-&#39;, label=&#39;inter-cluster&#39;) plt.plot(kRange, intra, &#39;ro-&#39;, label=&#39;intra-cluster&#39;) plt.legend(loc=&#39;upper right&#39;) plt.xlabel(&#39;k&#39;) plt.ylabel(&#39;Sum-of-squares&#39;) The “correct” number of clusters is approximately 3. This matches our expectation. Task Perform silhouette analysis on the clusters obtained with \\(k\\)-means for \\(k\\) = 2 to 5. What’s the suggested number of clusters? Show Solution Solution R Python library(cluster) # for silhouette function # Initialise some variables kRange &lt;- seq(from=2, to=5, by=1) avgWidth &lt;- rep(NA, length(kRange)) # Loop across desired range of ks for (k in kRange) { mdl &lt;- kmeans(x=xTrain , centers=k) silh &lt;- silhouette(mdl$cluster, dist(xTrain)) plot(silh, main=paste0(&#39;k=&#39;, k)) avgWidth[k-1] &lt;- mean(silh[, 3]) } # Plot average width as a function of k plot(kRange, avgWidth, type=&quot;o&quot;, pch=1, col=&quot;blue&quot;, lwd=3, lty=1, xlab=&quot;k&quot;, ylab=&quot;Average silhouette width&quot;) The “correct” number of clusters is approximately 3. This matches our expectation. However, note that the average silhouette width is quite low, suggesting that some wines are ambiguously assigned to a cluster, and that in general the clusters are not very homogenous. # Unfortunately in scikit-learn there is not a readily available function # to plot silhouettes, so we will have to write one ourselves # Adapted from scikit-learn: from sklearn.metrics import silhouette_samples, silhouette_score import numpy as np #===================================================================# def plot_silhouette(mdl, xTrain): &quot;&quot;&quot; Plots a silhouette plot for a k-means object mdl It also requires the training data xTrain &quot;&quot;&quot; # Compute the silhouette scores for all samples clustLabels = mdl.labels_ silhScores = silhouette_samples(xTrain, clustLabels) # Loop across all clusters hFig = plt.figure(figsize=(6,6)) yMin = 10 for i in range(mdl.n_clusters): # Aggregate scores for that cluster and sort them thisSilhScores = silhScores[clustLabels == i] thisSilhScores.sort() # Set plot limits and plot yMax = yMin + len(thisSilhScores) plt.fill_betweenx(np.arange(yMin, yMax), 0, thisSilhScores, facecolor=&#39;grey&#39;, edgecolor=&#39;black&#39;, alpha=0.7) plt.xlabel(&#39;Silhouette width $s_i$&#39;) plt.title(&#39;k={}, n={}\\n Average silhouette width: {:.2f}&#39;\\ .format(mdl.n_clusters, xTrain.shape[0], np.mean(silhScores))) # Leave space before plotting next cluster yMin = yMax + 10 # Show plot plt.show() return hFig #===================================================================# # Initialise some variables kRange = range(2, 6) avgWidth = np.empty(len(kRange)) # Loop across desired range of ks for k in kRange: mdl = KMeans(n_clusters=k).fit(X=xTrain) avgWidth[k-2] = silhouette_score(xTrain, mdl.labels_) plot_silhouette(mdl, xTrain) # Plot average width as a function of k plt.figure(figsize=(5,5)) plt.plot(kRange, avgWidth, &#39;bo-&#39;) plt.xlabel(&#39;k&#39;) plt.ylabel(&#39;Average silhouette width&#39;) The “correct” number of clusters is approximately 3. This matches our expectation. However, note that the average silhouette width is quite low, suggesting that some wines are ambiguously assigned to a cluster, and that in general the clusters are not very homogenous. Nowadays microarrays have been largely replaced by sequencing technologies. However, the problem remains exactly the same↩ "],
["dimensionality-reduction.html", "3 Dimensionality Reduction 3.1 Feature extraction", " 3 Dimensionality Reduction High-dimensional datasets are nowadays very common in science. Reducing the number of features per observation can provide several benefits: Elucidating the best predictors of the underlying process (plausible causal drivers under an experimental setup). Highlighting the data’s structure through visualisation. Improving the model’s predictive performance by removing uninformative features/extracting better features. Decreasing computational power. The rationale behind dimensionality reduction is straightforward: Although the data may seem high dimensional, the structure of the data can be represented by fewer features. This situation arises due to redundant features; driven by multicollinearity and/or covariates that are only weakly associated with the underlying process. Reducing the dimensionality of a problem is achieved through: Feature extraction: mapping the original data to a new feature set. Feature selection: selecting a subset of attributes. In the machine learning literature the term dimensionality reduction is commonly associated with (typically) unsupervised methods that transform high-dimensional data to a lower dimensional feature set, whilst feature selection is treated as part of the predictive modelling framework. This is because feature selection is generally performed in a supervised fashion (although some unsupervised methods do exist). Following this notion, feature selection will be discussed later on in the supervised learning section. 3.1 Feature extraction Analogous to representing complex and multifaceted concepts, such as biological diversity or health by using a diversity index or body mass index (BMI), feature extraction deals with finding “good” representations of high-dimensional data sets. For example, should we describe an image by individual pixel intensities or by extracting higher-order structures such as edges and shapes? The objective is to: Construct new features from the original measured variables that accentuate the inherent patterns in the data and are non-redundant. Feature extraction is a key step in machine learning; finding representations that are directly relevant to the task at hand (e.g. discriminating between two classes) will almost always result in better predictive accuracy than employing more complex models. Dimensionality reduction techniques aggregate dimensions together while trying to preserve as much of the data’s structure as possible. That is, observations that are “close” to each other remain so in the lower-dimensional projection. Here we introduce two popular “automatic” methods, PCA and t-SNE. Note, that the alternative to “automatic” methods is to “hand-craft” features, also known as feature engineering. This relies on expert knowledge to specify a set of potentially discriminatory features. For example, activity trackers (e.g Fitbit, Garmin Vivosport) rely on accelerometer data (the “raw” observed data) to recognise the current activity (e.g sleeping, running, walking). This rather noisy accelerometer data cannot be used directly. Instead a sliding time window is used to compute several time-domain (e.g statistical attributes such as mean and variance) and frequency-domain (e.g min/max frequencies, band power) features. 3.1.1 PCA Principal component analysis (PCA) is a linear dimensionality reduction method with widespread use. The new uncorrelated features (PCA 1, PCA 2,…) are weighted linear combinations of the original data: \\[ \\text{PCA 1} = w_{11}x_1 + w_{12}x_2 + \\ldots + w_{1p}x_p\\\\ \\text{PCA 2} = w_{21}x_1 + w_{22}x_2 + \\ldots + w_{2p}x_p\\\\ \\vdots\\\\ \\text{PCA p} = w_{p1}x_1 + w_{p2}x_2 + \\ldots + w_{pp}x_p \\] Where the \\(x\\)’s are the original features and \\(w\\)’s are the weights The weights are determined in such a way to find directions, called principal components, that maximise the variance of the data. Mathematically, this is the same as minimising the sum of squared distances from data points to their projections. This is also equivalent to maximising the trace and determinant of the covariance matrix of the projected data. For a data matrix \\(X \\in \\mathbb{R}^{n\\ \\mathrm{x}\\ p}\\), (\\(n\\): no. observations, \\(p\\): no. of covariates), the PCAs can be derived as follows: Compute covariance matrix \\(\\Sigma\\) on the standardised data. Compute eigenvectors/eigenvalues of covariance matrix \\(\\Sigma = U\\Lambda U^{-1}\\). Where\\(\\Lambda\\) is a diagonal matrix of eigenvalues and the columns in \\(U\\) are the eigenvectors. 2 Sort eigenvectors by decreasing eigenvalues and choose \\(k\\) eigenvectors with the largest eigenvalues. Use resultant \\(p \\times k\\) matrix to project \\(X\\) onto new subspace. The variance explained by the chosen \\(k\\)-components is \\(\\frac{\\sum_{i=1}^k\\lambda_i}{\\sum_{i=1}^D\\lambda_i} \\times 100\\%\\) Typically the first two or three components are used to plot the data in an attempt to reveal any groupings. R Python # Perform PCA and show summary mdl &lt;- prcomp(x=iris[-5]) summary(mdl) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 2.0563 0.49262 0.2797 0.15439 Proportion of Variance 0.9246 0.05307 0.0171 0.00521 Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 # Extract variance explained varExpl &lt;- round((mdl$sdev^2 / sum(mdl$sdev^2))*100, digits=1) # Visualise the first two PCs plot(mdl$x[, 1], mdl$x[, 2], pch=19, xlab=paste0(&#39;PCA 1: &#39;, varExpl[1], &#39;%&#39;), ylab=paste0(&#39;PCA 2: &#39;, varExpl[2], &#39;%&#39;)) # Perform PCA from sklearn.decomposition import PCA mdl = PCA(n_components=2) mdl.fit(X=iris.data) PCs = mdl.transform(X=iris.data) # extract PCs # Visualise the first two PCs plt.figure(figsize=(6, 5)) plt.scatter(PCs[:, 0], PCs[:, 1]) plt.xlabel(&#39;PCA 1: {:.1f}%&#39;.format(mdl.explained_variance_ratio_[0]*100)) plt.ylabel(&#39;PCA 2: {:.1f}%&#39;.format(mdl.explained_variance_ratio_[1]*100)) plt.show() Some of you might notice that the R and Python results have opposite signs. This is not a bug, but merely the fact that changing the sign of the components does not change the variance and neither their interpretion (see here for more details). Note: Even though we know that the Iris dataset is composed of three distinct species, the PCA plot is only accentuating two groups. This could be due to several reasons. For example, two species might be very close to each other and/or the linearity assumption is not satisfied. Recall that the objective of PCA is to maximise the variance of the data rather than finding clusters per se (see here for an extended discussion). 3.1.2 t-SNE t-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction technique. t-SNE finds a way to project the data into a lower-dimensional space/embedding such that the original high-dimensional clustering is preserved. The paper describing the algorithm is available here, however, I recommend watching this accessible presentation first, given by the author himself. Moreover, there’s this nice live demo that explains the effect of each hyperparameter on the t-SNE output. R Python # Compute t-SNE embedding library(Rtsne) set.seed(103) # to reproduce results mdl &lt;- Rtsne(X=iris[-5], dims=2, check_duplicates=FALSE) # Visualise results plot(mdl$Y[, 1], mdl$Y[, 2], pch=19, xlab=&#39;Embedding 1&#39;, ylab=&#39;Embedding 2&#39;) # Compute t-SNE embedding from sklearn.manifold import TSNE seed(103) # to reproduce results mdl = TSNE(n_components=2) embedding = mdl.fit_transform(X=iris.data) # Visualise the first two PCs plt.figure(figsize=(6, 5)) plt.scatter(embedding[:, 0], embedding[:, 1]) plt.xlabel(&#39;Embedding 1&#39;) plt.ylabel(&#39;Embedding 2&#39;) plt.show() Looks like t-SNE does a better job than PCA in seperating one cluster from another, and some might also argue that there’s some evidence for the presence of a third cluster. Let’s colour the data points by their respective species: R Python # PCA + t-SNE pca &lt;- prcomp(x=iris[, -5]) set.seed(103) # to reproduce results tsne &lt;- Rtsne(X=iris[, -5], dims=2, check_duplicates=FALSE) # Extract variance explained for PCA varExpl &lt;- round((pca$sdev^2 / sum(pca$sdev^2))*100, digits=1) # Assign colour to each species library(RColorBrewer) k &lt;- length(levels(iris[, 5])) # no. of true underlying clusters myCol &lt;- seq(k) names(myCol) &lt;- brewer.pal(n=k, &#39;Set1&#39;) # Plot results par(mfrow=c(1, 2)) # PCA plot(pca$x[, 1], pca$x[, 2], pch=19, xlab=paste0(&#39;PCA 1: &#39;, varExpl[1], &#39;%&#39;), ylab=paste0(&#39;PCA 2: &#39;, varExpl[2], &#39;%&#39;), col=names(myCol[as.numeric(iris[, 5])]), main=&#39;PCA&#39;) legend(&quot;topright&quot;, levels(iris[, 5]), bty=&quot;n&quot;, col=names(myCol), pch=19) # t-SNE plot(tsne$Y[, 1], tsne$Y[, 2], pch=19, xlab=&#39;Embedding 1&#39;, ylab=&#39;Embedding 2&#39;, col=names(myCol[as.numeric(iris[, 5])]), main=&#39;t-SNE&#39;) legend(&quot;topright&quot;, levels(iris[, 5]), bty=&quot;n&quot;, col=names(myCol), pch=19) import matplotlib.patches as mpatches # PCA + t-SNE pca = PCA(n_components=2).fit(X=iris.data) PCs = pca.transform(X=iris.data) # extract PCs seed(103) # to reproduce results tsne = TSNE(n_components=2).fit_transform(X=iris.data) # Assign colour to each species from palettable.colorbrewer.qualitative import Set1_3 colDict = {0: Set1_3.hex_colors[0], 1: Set1_3.hex_colors[1], 2: Set1_3.hex_colors[2]} myCol = [colDict[i] for i in iris.target] # Plot results hFig, hAx = plt.subplots(1, 2, figsize=(13,6)) # PCA hAx[0].scatter(PCs[:, 0], PCs[:, 1], c=myCol) hAx[0].set_xlabel(&#39;PCA 1: {:.1f}%&#39;.format(pca.explained_variance_ratio_[0]*100)) hAx[0].set_ylabel(&#39;PCA 2: {:.1f}%&#39;.format(pca.explained_variance_ratio_[1]*100)) hAx[0].set_title(&#39;PCA&#39;) legend = [] for i, species in enumerate(iris.target_names): legend.append(mpatches.Patch(color=colDict[i], label=species)) hAx[0].legend(loc=&#39;upper center&#39;, handles=legend) # t-SNE hAx[1].scatter(tsne[:, 0], tsne[:, 1], c=myCol) hAx[1].set_xlabel(&#39;Embedding 1&#39;) hAx[1].set_ylabel(&#39;Embedding 2&#39;) hAx[1].set_title(&#39;t-SNE&#39;) legend = [] for i, species in enumerate(iris.target_names): legend.append(mpatches.Patch(color=colDict[i], label=species)) hAx[1].legend(loc=&#39;lower right&#39;, handles=legend) plt.show() Although t-SNE does a better job at seperating setosa from the rest and creates tighter clusters, it’s still hard to tell versicolor and virginica apart in the absence of their label (although these groups are better defined in the t-SNE plot). As discussed in the previous clustering section, this is a shortcoming of unsupervised learning methods, that is, we can never be sure about the “true” underlying number of groups. Nevertheless, these dimensionality reduction techniques are incredibly useful at visualising high-dimensional datasets and uncover global structure. \\(\\Sigma\\) must be postive semi-definite. The singular value decomposition (SVD) is usually preferred over eigendecomposition as it’s more numerically stable↩ "],
["supervised-learning-1.html", "4 Supervised Learning 4.1 Motivation 4.2 What is supervised learning? 4.3 What problems can supervised learning solve? 4.4 Cross-validation 4.5 Predictive performance measures 4.6 \\(k\\)-nearest neighbour (\\(k\\)NN) 4.7 Decision trees 4.8 Random forests 4.9 Support vector machines (SVM) 4.10 Tasks", " 4 Supervised Learning 4.1 Motivation The picture above was taken from here and depicts the great wildebeest migration from Tanzania’s Serengeti national park to the south of Kenya’s Masai Mara national park. This migration is of great ecological importance. Conservation biologists are particularly interested in estimating the population of wildebeest and observe how it changes over time. This is typically done through aerial surveys. The result is several thousands of images that an expert need to count manually. This process is of course painstakingly slow and prone to human error. Instead we can segment each image and train a machine learning (ML) algorithm to identify different classes (see here and here). The graph below shows carbon dioxide concentration (CO\\(_2\\)) over time measured at the Mauna Loa observatory in Hawaii. The data exhibits seasonal oscillations, together with an increasing trend. Such complex behaviour cannot be captured with classical linear models. Instead supervised learning methods (e.g Gaussian Processes) can be used to learn this complex time-series in order to perform predictions to inform climate change policies, for example. 4.2 What is supervised learning? Akin to traditional statistical models (e.g. generalised linear models (GLMs)), supervised learning methods determine the mapping (predictive model) between a set of features and a continuous outcome (regression), or a categorical variable (classification). The observed data is split into a training set, which is used to build the predictive model, whilst the testing data set (not used in model building) is used to compute the expected predictive performance “in the field”. In statistics, this is similar to making inferences about the population based on a finite and random sample. 4.3 What problems can supervised learning solve? Medical imaging: identifying a tumour as being benign or cancerous. Gene expression: determining a patient’s phenotype based on their gene expression “signature”. Computer vision: detecting and tracking a moving object. Biogeography: predicting land cover usage using remote sensing imagery. Speech recognition: translating audio signals into written text. Biometric authentication: identifying a person using their fingerprint. Epidemiology: predicting the likelihood of an individual to develop a particular disease, given a number of risk factors. … and much more! 4.4 Cross-validation ML algorithms can deal with nonlinearities and complex interactions amongst variables because the models are flexible enough to fit the data (as opposed to rigid linear regression models, for example). However, this flexibility needs to be constrained to avoid fitting to noise (overfitting). This is achieved by tuning the model’s hyperparameters. Hyperparameters are parameters that are not directly learnt by the machine learning algorithm, but affect its structure. For example, consider a simple polynomial regression model: \\[ y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\ldots + \\beta_px^p \\] The \\(\\beta\\)’s are the model parameters that are inferred/learnt from the data. The degree of the polynomial \\(p\\), however, is a hyperparameter that dictates the complexity of the model. Hyperparameters are tuned by cross-validation to strike a balance between underfitting and overfitting, known as the bias-variance trade-off (Fig. 4.1a). Hyperparameter tuning is essentially a form of model selection. Compared to statistical modelling, information criterions and \\(p\\)-values are replaced by predictive performance measures. Note that in the statistics literature, model selection tends to encompass every aspect of choosing the final model (i.e model structure and which variables and interaction terms to keep). In ML, model selection (the structure of the model) and feature selection (which covariates to keep in the model) tend to be treated separately. Nevertheless, typically, both require some form of cross-validation. Figure 4.1: Cross-validation In \\(k\\)-fold cross-validation the training data are randomly split into \\(k\\) parts. The model is trained on all but one of the folds, and performance is measured on the part left out in the training process (Fig. 4.1b). The average prediction error is computed from the \\(k\\) runs and the hyperparameters that minimise this error are used to build the final model (Fig. 4.1c). To make cross-validation insensitive to a single random partitioning of the data, repeated cross-validation is typically performed, where cross-validation is repeated on several random splits of the data. 4.5 Predictive performance measures In order to perform cross-validation, specifically to compare models with different hyperparameters, we need to evaluate how good a model is. There are several predictive performance measures available in the literature that we can use to this end. Some of the more popular ones are: Regression: root mean squared error (RMSE), R-squared Classification: area uder the receiver operating characteristic (ROC) curve, confusion matrix Next, we present a few popular supervised learning methods, focusing on classification problems (although most methods can tackle regression too). A rather exhaustive list of ML algorithms can be found in the caret package, for R users, and the scikit-learn package, for Python users. I strongly recommend reading the user guide of these packages to familiarise yourself with their interface. Once you choose a particular ML algorithm, make sure to familiarise yourself with its tuning parameters. 4.6 \\(k\\)-nearest neighbour (\\(k\\)NN) Arguably the simplest model available and typically used as a baseline to benchmark other ML algorithms. The rationale behind \\(k\\)NN is simple; the class label for a particular test point is the majority vote of the surrounding training data: Compute the distance between test point and every training data point. Find the \\(k\\) training points closest to the test point. Assign test point the majority vote of their class label. R Python library(caret) # Split test/train set.seed(103) # for reproducibility ii &lt;- createDataPartition(iris[, 5], p=.7, list=F) ## returns indices for train data xTrain &lt;- iris[ii, 1:4]; yTrain &lt;- iris[ii, 5] xTest &lt;- iris[-ii, 1:4]; yTest &lt;- iris[-ii, 5] dim(xTrain) [1] 105 4 dim(xTest) [1] 45 4 # Set training options # Repeat 5-fold cross-validation, ten times opts &lt;- trainControl(method=&#39;repeatedcv&#39;, number=5, repeats=10, p=0.7) # Find optimal k (model) set.seed(1040) # for reproducibility mdl &lt;- train(x=xTrain, y=yTrain, # training data method=&#39;knn&#39;, # machine learning model trControl=opts, # training options tuneGrid=data.frame(k=seq(2, 15))) # range of k&#39;s to try print(mdl) k-Nearest Neighbors 105 samples 4 predictor 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; No pre-processing Resampling: Cross-Validated (5 fold, repeated 10 times) Summary of sample sizes: 84, 84, 84, 84, 84, 84, ... Resampling results across tuning parameters: k Accuracy Kappa 2 0.9504762 0.9257143 3 0.9600000 0.9400000 4 0.9552381 0.9328571 5 0.9619048 0.9428571 6 0.9666667 0.9500000 7 0.9666667 0.9500000 8 0.9666667 0.9500000 9 0.9676190 0.9514286 10 0.9685714 0.9528571 11 0.9638095 0.9457143 12 0.9638095 0.9457143 13 0.9628571 0.9442857 14 0.9638095 0.9457143 15 0.9676190 0.9514286 Accuracy was used to select the optimal model using the largest value. The final value used for the model was k = 10. # Test model on testing data yTestPred &lt;- predict(mdl, newdata=xTest) confusionMatrix(yTestPred, yTest) # predicted/true Confusion Matrix and Statistics Reference Prediction setosa versicolor virginica setosa 15 0 0 versicolor 0 14 0 virginica 0 1 15 Overall Statistics Accuracy : 0.9778 95% CI : (0.8823, 0.9994) No Information Rate : 0.3333 P-Value [Acc &gt; NIR] : &lt; 2.2e-16 Kappa : 0.9667 Mcnemar&#39;s Test P-Value : NA Statistics by Class: Class: setosa Class: versicolor Class: virginica Sensitivity 1.0000 0.9333 1.0000 Specificity 1.0000 1.0000 0.9667 Pos Pred Value 1.0000 1.0000 0.9375 Neg Pred Value 1.0000 0.9677 1.0000 Prevalence 0.3333 0.3333 0.3333 Detection Rate 0.3333 0.3111 0.3333 Detection Prevalence 0.3333 0.3111 0.3556 Balanced Accuracy 1.0000 0.9667 0.9833 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import RepeatedKFold, GridSearchCV from sklearn.metrics import confusion_matrix # Split test/train xTrain, xTest, yTrain, yTest = train_test_split(iris.data, iris.target, train_size=0.7, random_state=103) print(xTrain.shape) (105, 4) print(xTest.shape) (45, 4) cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=1040) # Repeat 5-fold cross-validation, ten times mdl = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={&#39;n_neighbors&#39;: range(2, 16)}, cv=cv) # Set search grid for k # Find optimal k (model) mdl.fit(X=xTrain, y=yTrain) GridSearchCV(cv=RepeatedKFold(n_repeats=10, n_splits=5, random_state=1040), error_score=nan, estimator=KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;), iid=&#39;deprecated&#39;, n_jobs=None, param_grid={&#39;n_neighbors&#39;: range(2, 16)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) print(mdl.best_estimator_) KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) yTestPred = mdl.predict(xTest) # evaluate performance on test data print(confusion_matrix(yTest, yTestPred)) # true/predicted [[14 0 0] [ 0 13 0] [ 0 0 18]] Pros Cons Simple and intuitive Can be computationally expensive, as for every test point, distance to every training data point needs to be computed Works for multi-class problems Takes up a lot of storage as all training points need to be retained Non-linear decision boundaries \\(k\\) easily tuned by cross-validation 4.7 Decision trees Decision trees are simple and intuitive predictive models, making them a popular choice when decision rules are required, for example in medicine. A decision tree is constructed as follows: Find the yes/no rule that best splits the data with respect to one of the features. The best split is the one that produces the most homogeneous groups; found by maximising information gain/lowering entropy. Repeat steps 1 to 2 until all data are correctly classified or some stopping rule reached. R Python library(C50) # https://topepo.github.io/C5.0/ # Fit and plot model mdl &lt;- C5.0(x=xTrain, y=yTrain) plot(mdl) # Test model on testing data yTestPred &lt;- predict(mdl, newdata=xTest) confusionMatrix(yTestPred, yTest) # predicted/true Confusion Matrix and Statistics Reference Prediction setosa versicolor virginica setosa 15 0 0 versicolor 0 14 0 virginica 0 1 15 Overall Statistics Accuracy : 0.9778 95% CI : (0.8823, 0.9994) No Information Rate : 0.3333 P-Value [Acc &gt; NIR] : &lt; 2.2e-16 Kappa : 0.9667 Mcnemar&#39;s Test P-Value : NA Statistics by Class: Class: setosa Class: versicolor Class: virginica Sensitivity 1.0000 0.9333 1.0000 Specificity 1.0000 1.0000 0.9667 Pos Pred Value 1.0000 1.0000 0.9375 Neg Pred Value 1.0000 0.9677 1.0000 Prevalence 0.3333 0.3333 0.3333 Detection Rate 0.3333 0.3111 0.3333 Detection Prevalence 0.3333 0.3111 0.3556 Balanced Accuracy 1.0000 0.9667 0.9833 from sklearn import tree import graphviz # Fit model mdl = tree.DecisionTreeClassifier() mdl.fit(X=xTrain, y=yTrain) # Plot model using graphviz DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;, random_state=None, splitter=&#39;best&#39;) mdlStr = tree.export_graphviz(mdl, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) # export model as a string graph = graphviz.Source(mdlStr) graph.render(&#39;iris_tree&#39;) # save tree as a pdf &#39;iris_tree.pdf&#39; yTestPred = mdl.predict(xTest) # evaluate performance on test data print(confusion_matrix(yTest, yTestPred)) # true/predicted [[14 0 0] [ 0 13 0] [ 0 1 17]] Pros Cons Model is very easy to explain to non-experts and can be directly used to generate rules Can easily overfit the data Computationaly inexpensive to train, evaluate and store Predictive accuracy can be poor Handle both categorical and continuous data Linear decision boundaries Robust to outliers Small changes to training data may lead to a completely different tree 4.8 Random forests Random forests is an ensemble method developed to mitigate the problem of overfitting in decision trees. Instead of a single tree, multiple decision trees are grown and averaged over as follows (each tree is known as a weak learner): Grow \\(T\\) decorrelated trees (no pruning). Induce randomness by: Bagging (bootstrap aggregating), where each tree is trained on a subset of the data randomly sampled with replacement. Considering only a subset of predictors as candidates for each split. Average predictions from all \\(T\\) trees. Cross-validation is inherent in the random forests methodology as every tree is trained only on a subset of the original data. This allows the computation of an estimate for the generalisation error by computing the predictive performance of the model on the data left out from the training process, known as the out-of- bag (OOB) error. The OOB data are also used to compute an estimate of the importance of every predictor, which can be subsequently used for feature selection. R Python # Fit Random Forest model # Fix ntree and mtry set.seed(1040) # for reproducibility mdl &lt;- train(x=xTrain, y=yTrain, method=&#39;rf&#39;, ntree=200, tuneGrid=data.frame(mtry=2)) print(mdl) Random Forest 105 samples 4 predictor 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 105, 105, 105, 105, 105, 105, ... Resampling results: Accuracy Kappa 0.941297 0.9098626 Tuning parameter &#39;mtry&#39; was held constant at a value of 2 # Test model on testing data yTestPred &lt;- predict(mdl, newdata=xTest) confusionMatrix(yTestPred, yTest) # predicted/true Confusion Matrix and Statistics Reference Prediction setosa versicolor virginica setosa 15 0 0 versicolor 0 14 0 virginica 0 1 15 Overall Statistics Accuracy : 0.9778 95% CI : (0.8823, 0.9994) No Information Rate : 0.3333 P-Value [Acc &gt; NIR] : &lt; 2.2e-16 Kappa : 0.9667 Mcnemar&#39;s Test P-Value : NA Statistics by Class: Class: setosa Class: versicolor Class: virginica Sensitivity 1.0000 0.9333 1.0000 Specificity 1.0000 1.0000 0.9667 Pos Pred Value 1.0000 1.0000 0.9375 Neg Pred Value 1.0000 0.9677 1.0000 Prevalence 0.3333 0.3333 0.3333 Detection Rate 0.3333 0.3111 0.3333 Detection Prevalence 0.3333 0.3111 0.3556 Balanced Accuracy 1.0000 0.9667 0.9833 # Variable importance by mean decrease in gini index varImp(mdl$finalModel) Overall Sepal.Length 6.912099 Sepal.Width 1.943424 Petal.Length 31.290924 Petal.Width 29.105553 from sklearn.ensemble import RandomForestClassifier # Fit Random Forest model # Fix ntree and mtry mdl = RandomForestClassifier(n_estimators=200, max_features=2, random_state=1040) mdl.fit(X=xTrain, y=yTrain) RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=2, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None, oob_score=False, random_state=1040, verbose=0, warm_start=False) yTestPred = mdl.predict(xTest) # evaluate performance on test data print(confusion_matrix(yTest, yTestPred)) # true/predicted # Variable importance by mean decrease in gini index [[14 0 0] [ 0 13 0] [ 0 0 18]] print(iris.feature_names) # print to remind us the order of features [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] print(mdl.feature_importances_) [0.09896118 0.03103976 0.4213009 0.44869816] Pros Cons State-of-the-art predictive accuracy Harder to interpret then plain decision trees Can handle thousands of both categorical and continuous predictors without variable deletion Robust to outliers Estimates the importance of every predictor Out-of-bag error (unbiased estimate of test error for every tree built) Copes with unbalanced datasets by setting class weights Trivially parallelisable 4.9 Support vector machines (SVM) All the grey lines in the GIF above do a good job at seperating the “blue” and “red” points. But which line is the “best” at seperating these two classes? The rationale behind a maximal margin classifier is to find an optimal line/hyperplane that maximises the margin, that is, the distance between data points of both classes. This turns out to be a rather straightforward optimisation problem. But what do we do if there isn’t a “clean” separating line between the classes? Support vector classifiers (SVC) were developed that use a soft margin approach. The hyperplane is placed in a way that it correctly classifies most of the data points. In reality, we face even more complex data sets where a hyperplane would never do a good job at separating the two classes. For example: We can see that a non-linear boundary would do the job. Support vector machines are a generalisation of support vector classifiers that make use of kernels to map the original feature set to a higher dimensional space where classes are linearly separable. This might sound counter-intuitive, as increasing the dimensionality of the problem is undesireable. However, the kernel trick enable us to work in an implicit feature space, such that the data is never explicitly expressed in higher dimensions. Think about kernels as generalised distance measures. The type of kernel is a hyperparameter that we can infer using cross-validation. However, in caret, each kernel is defined as a separate model, and thus the cross-validation loop need to be written manually rather than relying on the trainControl function. This is not a problem in scikit-learn where SVMs are implemented as a generic function that takes kernel as an input. Note: SVMs are inherently binary classifiers. The most common ways to deal with multi-class problems is by building several one-versus-all or one-versus-one classifiers. R Python # Set training options # Repeat 5-fold cross-validation, ten times opts &lt;- trainControl(method=&#39;repeatedcv&#39;, number=5, repeats=10, p=0.7) # Fit SVM set.seed(1040) # for reproducibility mdl &lt;- train(x=xTrain, y=yTrain, # training data method=&#39;svmLinear&#39;, # machine learning model trControl=opts, # training options tuneGrid=data.frame(C=c(0.01, 1, 10, 100, 1000))) # range of C&#39;s to try print(mdl) Support Vector Machines with Linear Kernel 105 samples 4 predictor 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; No pre-processing Resampling: Cross-Validated (5 fold, repeated 10 times) Summary of sample sizes: 84, 84, 84, 84, 84, 84, ... Resampling results across tuning parameters: C Accuracy Kappa 1e-02 0.8371429 0.7557143 1e+00 0.9628571 0.9442857 1e+01 0.9628571 0.9442857 1e+02 0.9828571 0.9742857 1e+03 0.9752381 0.9628571 Accuracy was used to select the optimal model using the largest value. The final value used for the model was C = 100. # Test model on testing data yTestPred &lt;- predict(mdl, newdata=xTest) confusionMatrix(yTestPred, yTest) # predicted/true Confusion Matrix and Statistics Reference Prediction setosa versicolor virginica setosa 15 0 0 versicolor 0 14 1 virginica 0 1 14 Overall Statistics Accuracy : 0.9556 95% CI : (0.8485, 0.9946) No Information Rate : 0.3333 P-Value [Acc &gt; NIR] : &lt; 2.2e-16 Kappa : 0.9333 Mcnemar&#39;s Test P-Value : NA Statistics by Class: Class: setosa Class: versicolor Class: virginica Sensitivity 1.0000 0.9333 0.9333 Specificity 1.0000 0.9667 0.9667 Pos Pred Value 1.0000 0.9333 0.9333 Neg Pred Value 1.0000 0.9667 0.9667 Prevalence 0.3333 0.3333 0.3333 Detection Rate 0.3333 0.3111 0.3111 Detection Prevalence 0.3333 0.3333 0.3333 Balanced Accuracy 1.0000 0.9500 0.9500 from sklearn.svm import SVC cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=1040) # Repeat 5-fold cross-validation, ten times paramGrid = [{&#39;kernel&#39;: [&#39;rbf&#39;], &#39;gamma&#39;: [1e-3, 1e-4], &#39;C&#39;: [1, 10, 100, 1000]}, {&#39;kernel&#39;: [&#39;linear&#39;], &#39;C&#39;: [1, 10, 100, 1000]}] # set hyperprameter search grid mdl = GridSearchCV(estimator=SVC(), param_grid=paramGrid, cv=cv) # Fit SVM mdl.fit(X=xTrain, y=yTrain) GridSearchCV(cv=RepeatedKFold(n_repeats=10, n_splits=5, random_state=1040), error_score=nan, estimator=SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), iid=&#39;deprecated&#39;, n_jobs=None, param_grid=[{&#39;C&#39;: [1, 10, 100, 1000], &#39;gamma&#39;: [0.001, 0.0001], &#39;kernel&#39;: [&#39;rbf&#39;]}, {&#39;C&#39;: [1, 10, 100, 1000], &#39;kernel&#39;: [&#39;linear&#39;]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) print(mdl.best_estimator_) SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=0.001, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) yTestPred = mdl.predict(xTest) # evaluate performance on test data print(confusion_matrix(yTest, yTestPred)) # true/predicted [[14 0 0] [ 0 13 0] [ 0 0 18]] Pros Cons State-of-the-art predictive accuracy Model is hard to interpret Low storage requirements (only the support vectors need to be stored) Feature space cannot be visualised A vast array of kernels are available that are flexible enough to cater for any type of data Global optimum guaranteed 4.10 Tasks If you haven’t already, download the gene expression and wine datasets from here. 4.10.1 Gene expression Task Use PCA and t-SNE to visualise the dataset (colour each observation by cancer type and stage) Use any of the techniques described above (feel free to try and compare all of them) and build a binary classifier to classify the B- and T-cell leukaemia patients. Compute the predictive performance measures and visualise the results as a ROC curve 4.10.2 Wine Task Use PCA and t-SNE to visualise the dataset (colour each observation by wine type) Use any of the techniques described above (feel free to try and compare all of them) and build a multi-label classifier to classify the three different types of wine Compute the predictive performance measures and visualise the results as a confusion matrix 4.10.3 UCI Machine Learning Repository The UCI repository contains a collection of datasets that span different fields. I encourage you to choose a handful of datasets, maybe ones related to your research area, and practice further fitting machine learning models. If you’re familiar with literate programming (if not, see TJ McKinley’s course or Jupyter for Python users), I suggest you document every operation performed on the dataset: cleaning, normalisation/standardisation, visualisation, feature extraction, model fitting and model evaluation. "]
]
